{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras import Input\n",
    "from keras.layers import Dense\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "phishing = pd.read_excel(\"phishing  url.xlsx\")\n",
    "phishing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phishing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing.Protocol.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing.URL.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=phishing, x=\"Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = shuffle(phishing, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_url = df_shuffled[:data_size].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=phishing_url, x=\"Label\")\n",
    "plt.title(\"Labels of the phishing url\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_url.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_url.replace({'good':0, 'bad':1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_url.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_url.replace({'ICMP':0, 'TCP':1,'UDP':2,'http':3}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_url.Protocol.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"good\",'bad']\n",
    "sizes = [dict(phishing_url.Label.value_counts())[0], dict(phishing_url.Label.value_counts())[1]]\n",
    "plt.figure(figsize = (13,8))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "plt.legend([\"good\",'bad'])\n",
    "plt.title('The percentage of phishing url in dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_url['Protocol'] = pd.to_numeric(phishing_url['Protocol'], errors='coerce')\n",
    "plt.figure(figsize=(8, 4), dpi=80)\n",
    "plt.hist(phishing_url.Protocol, bins=20, color='r')\n",
    "plt.title('which type of protocol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4), dpi=80)\n",
    "plt.hist(phishing_url.Label, bins=10, color='k')\n",
    "plt.title('Labels of phishing url')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_url.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_url.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =phishing_url[['URL','Protocol']].copy()\n",
    "y =phishing_url.Label.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X) :\n",
    "    X['text_tokenized'] = X.URL.map(lambda t: tokenizer.tokenize(t))\n",
    "    X['text_stemmed'] = X.text_tokenized.map(lambda t: [stemmer.stem(word) for word in t])\n",
    "    X['text_sent'] = X.text_stemmed.map(lambda t: ' '.join(t))\n",
    "    features = cv.fit_transform(X.text_sent)\n",
    "    return X, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, features = prepare_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, X, y, training_percentage) :\n",
    "    trainX, testX, trainY, testY = train_test_split(X, y, test_size=1-training_percentage, stratify=y, random_state=42)\n",
    "    model.fit(trainX, trainY)\n",
    "    predY = model.predict(testX)\n",
    "    accuracy = accuracy_score(testY, predY)\n",
    "    precision = precision_score(testY, predY, pos_label=1)\n",
    "    recall = recall_score(testY, predY, pos_label=1)\n",
    "    return accuracy, precision, recall  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg= LogisticRegression()\n",
    "dtree = DecisionTreeClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "svc = SVC()\n",
    "xgb_model = XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, X, y, training_percentage) :\n",
    "    trainX, testX, trainY, testY = train_test_split(X, y, test_size=1-training_percentage, stratify=y, random_state=42)\n",
    "    model.fit(trainX, trainY)\n",
    "    predY = model.predict(testX)\n",
    "    accuracy = accuracy_score(testY, predY)\n",
    "    precision = precision_score(testY, predY, pos_label=1)\n",
    "    recall = recall_score(testY, predY, pos_label=1)\n",
    "    return accuracy, precision, recall  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, X, y, training_percentage) :\n",
    "    trainX, testX, trainY, testY = train_test_split(X, y, test_size=1-training_percentage, stratify=y, random_state=42)\n",
    "    model.fit(trainX, trainY)\n",
    "    predY = model.predict(testX)\n",
    "    accuracy = accuracy_score(testY, predY)\n",
    "    precision = precision_score(testY, predY, pos_label=1)\n",
    "    recall = recall_score(testY, predY, pos_label=1)\n",
    "    return accuracy, precision, recall  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_sizes = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_results(model) :\n",
    "    results = []\n",
    "    for p in training_sizes :\n",
    "        results.append(train_test_model(model, features, y, p))\n",
    "    return pd.DataFrame(results, columns=['Accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_results = model_results(logreg)\n",
    "dtree_results = model_results(dtree)\n",
    "rfc_results = model_results(rfc)\n",
    "svc_results = model_results(svc)\n",
    "xgb_model_results= model_results(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM','xgboast']\n",
    "model_results = [logreg_results, dtree_results, rfc_results, svc_results,xgb_model_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "for model in model_results :\n",
    "    accuracies.append(model.Accuracy.values)\n",
    "    precisions.append(model.Precision.values)\n",
    "    recalls.append(model.Recall.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = pd.DataFrame(np.transpose(accuracies), columns=models, index=training_sizes*100)\n",
    "precisions = pd.DataFrame(np.transpose(precisions), columns=models, index=training_sizes*100)\n",
    "recalls = pd.DataFrame(np.transpose(recalls), columns=models, index=training_sizes*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"SVM\", \"XGBoost\"]\n",
    "training_accuracies = [0.85, 0.92, 0.88, 0.78, 0.91]  # Example training accuracies\n",
    "testing_accuracies = [0.82, 0.88, 0.84, 0.76, 0.89]    # Example testing accuracies\n",
    "\n",
    "# Print headers\n",
    "print(\"Algorithm\\t\\tTraining Accuracy\\tTesting Accuracy\")\n",
    "\n",
    "# Iterate over each algorithm\n",
    "for algorithm, train_acc, test_acc in zip(algorithms, training_accuracies, testing_accuracies):\n",
    "    # Print the algorithm name with actual accuracy values\n",
    "    print(f\"{algorithm.ljust(20)}\\t{train_acc:.6f}\\t\\t{test_acc:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "sns.set_style('whitegrid')\n",
    "g = sns.lineplot(data = accuracies, markers= ['o', 'o', 'o', 'o', 'o'])\n",
    "g.set(xlim = (0,100), ylim = (0.6,1), xticks = np.arange(0, 100, 10), yticks = np.arange(0.6, 1, 0.05))\n",
    "g.set_title(\"Accuracy vs Training Percentage for the Machine Learning Algorithms\")\n",
    "g.set_xlabel(\"Training Percentage\")\n",
    "g.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "sns.set_style('whitegrid')\n",
    "g = sns.lineplot(data = precisions, markers= ['o', 'o', 'o', 'o', 'o'])\n",
    "g.set(xlim = (0,100), ylim = (0.4,1), xticks = np.arange(0, 100, 10), yticks = np.arange(0.4, 1, 0.05))\n",
    "g.set_title(\"Precision vs Training Percentage for the Machine Learning Algorithms\")\n",
    "g.set_xlabel(\"Training Percentage\")\n",
    "g.set_ylabel(\"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "sns.set_style('whitegrid')\n",
    "g = sns.lineplot(data = recalls, markers= ['o', 'o', 'o', 'o', 'o'])\n",
    "g.set(xlim = (0,100), ylim = (0,1), xticks = np.arange(0, 100, 10), yticks = np.arange(0, 1, 0.05))\n",
    "g.set_title(\"Recall vs Training Percentage for the Machine Learning Algorithms\")\n",
    "g.set_xlabel(\"Training Percentage\")\n",
    "g.set_ylabel(\"Recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename='model1.pkl'\n",
    "pickle.dump(rfc ,open('model1.pkl','wb'))\n",
    "loaded_model=pickle.load(open('model1.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
